{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4af5acOTsUKc",
        "outputId": "5c8b1780-9965-4fc7-ea34-6bae28ec4667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "%pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wuBHgw0RtIg_"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lyi9WeC2unl1"
      },
      "outputs": [],
      "source": [
        "SID = \"20862105\"\n",
        "student_name = \"Lam Leung Kin\"\n",
        "num_merges = (int(SID[-1]) + 1) * 100\n",
        "input_path = \"./text_news.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQTPcagKwurX",
        "outputId": "12c06775-6325-4e54-efd1-cf3712453ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "600\n"
          ]
        }
      ],
      "source": [
        "print(num_merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WHKOaTPqs_Xi"
      },
      "outputs": [],
      "source": [
        "# Train the BPE model\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input = input_path,\n",
        "    model_prefix = 'bpe_model',\n",
        "    vocab_size = 1000,\n",
        "    num_threads = num_merges,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reWGvri2wE7L",
        "outputId": "89616156-82e0-4ade-c6cf-6bcf532bc7b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the trained model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('bpe_model.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xGpFgBOwHxC",
        "outputId": "568d2b48-8490-4b88-9ef8-4e7cb2d89918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Name: ['▁La', 'm', '▁L', 'e', 'un', 'g', '▁K', 'in']\n"
          ]
        }
      ],
      "source": [
        "tokenized_name = sp.encode_as_pieces(student_name)\n",
        "print(\"Tokenized Name:\", tokenized_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8fHvCFbwaTN",
        "outputId": "fee99478-5be7-49d4-e683-8eec8ae2f058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized text saved to: tokenized_text_news.txt\n"
          ]
        }
      ],
      "source": [
        "output_file = \"tokenized_text_news.txt\"\n",
        "\n",
        "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text using the BPE model\n",
        "tokenized_text = sp.encode_as_pieces(text)\n",
        "\n",
        "# Save the tokenized text to a new file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join(tokenized_text))\n",
        "\n",
        "print(\"Tokenized text saved to:\", output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
