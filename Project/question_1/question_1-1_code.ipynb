{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4af5acOTsUKc",
        "outputId": "862bab7b-9bce-4c03-c9d2-504175784e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "%pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wuBHgw0RtIg_"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lyi9WeC2unl1"
      },
      "outputs": [],
      "source": [
        "SID = \"20862105\"\n",
        "student_name = \"Lam Leung Kin\"\n",
        "num_merges = (int(SID[-1]) + 1) * 100\n",
        "input_path = \"./text_news.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQTPcagKwurX",
        "outputId": "8ef7e8f1-2680-4db5-d6c1-db57302e6848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "600\n"
          ]
        }
      ],
      "source": [
        "print(num_merges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WHKOaTPqs_Xi"
      },
      "outputs": [],
      "source": [
        "# Train the BPE model\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input = input_path,\n",
        "    model_prefix = 'bpe_model',\n",
        "    vocab_size = num_merges,\n",
        ")\n",
        "\n",
        "# vocab_size: represents the maximum number of unique tokens that will be present in the vocabulary.\n",
        "#   Each merge in the learner will create a new token in the vocabulary while it might not be unique as it could be added from the early learning\n",
        "#   , but we assume vocab_size = num_merges for simiplicity and they did have linear relationship in the early learning phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reWGvri2wE7L",
        "outputId": "c0c47661-1d5f-4e3d-f2bb-b2bfd2d4025f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the trained model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('bpe_model.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xGpFgBOwHxC",
        "outputId": "137a9c00-28fb-4db4-cbce-e235bc079038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized Name: ['▁L', 'a', 'm', '▁L', 'e', 'un', 'g', '▁K', 'in']\n"
          ]
        }
      ],
      "source": [
        "tokenized_name = sp.encode_as_pieces(student_name)\n",
        "print(\"Tokenized Name:\", tokenized_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8fHvCFbwaTN",
        "outputId": "43955079-bdfe-44ed-94f9-77231b688084"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized text saved to: tokenized_text_news.txt\n"
          ]
        }
      ],
      "source": [
        "output_file = \"tokenized_text_news.txt\"\n",
        "\n",
        "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text using the BPE model\n",
        "tokenized_text = sp.encode_as_pieces(text)\n",
        "\n",
        "# Save the tokenized text to a new file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\" \".join(tokenized_text))\n",
        "\n",
        "print(\"Tokenized text saved to:\", output_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
